{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20 epochs, 29 batches each ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamw\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "C:\\Users\\adamw\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([15000])) that is different to the input size (torch.Size([15000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "C:\\Users\\adamw\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([10000])) that is different to the input size (torch.Size([10000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 0.4113349914550781 - acc: 0.9067333333333333 - loss_val: 0.4452159106731415 - acc_val: 0.8703\n",
      "Epoch 2 - loss: 0.21281594038009644 - acc: 0.9392666666666667 - loss_val: 0.29257526993751526 - acc_val: 0.8903\n",
      "Epoch 3 - loss: 0.15908195078372955 - acc: 0.9534 - loss_val: 0.27895647287368774 - acc_val: 0.8897\n",
      "Epoch 4 - loss: 0.12361491471529007 - acc: 0.9659333333333333 - loss_val: 0.27667757868766785 - acc_val: 0.8902\n",
      "Epoch 5 - loss: 0.10163246840238571 - acc: 0.974 - loss_val: 0.2920284569263458 - acc_val: 0.885\n",
      "Epoch 6 - loss: 0.08374352008104324 - acc: 0.9792 - loss_val: 0.30922406911849976 - acc_val: 0.8826\n",
      "Epoch 7 - loss: 0.06579062342643738 - acc: 0.9855333333333334 - loss_val: 0.3244576156139374 - acc_val: 0.8825\n",
      "Epoch 8 - loss: 0.0579681396484375 - acc: 0.9868 - loss_val: 0.35587945580482483 - acc_val: 0.88\n",
      "Epoch 9 - loss: 0.08667836338281631 - acc: 0.9672666666666667 - loss_val: 0.45202934741973877 - acc_val: 0.8615\n",
      "Epoch 10 - loss: 0.03767857700586319 - acc: 0.9934 - loss_val: 0.3950328230857849 - acc_val: 0.8782\n",
      "Epoch 11 - loss: 0.030569422990083694 - acc: 0.9951333333333333 - loss_val: 0.41592714190483093 - acc_val: 0.8784\n",
      "Epoch 12 - loss: 0.025098877027630806 - acc: 0.9962 - loss_val: 0.4402294456958771 - acc_val: 0.8761\n",
      "Epoch 13 - loss: 0.02080381102859974 - acc: 0.9973333333333333 - loss_val: 0.46490877866744995 - acc_val: 0.8758\n",
      "Epoch 14 - loss: 0.017512347549200058 - acc: 0.9978666666666667 - loss_val: 0.49327167868614197 - acc_val: 0.8741\n",
      "Epoch 15 - loss: 0.01502580102533102 - acc: 0.9980666666666667 - loss_val: 0.5213311910629272 - acc_val: 0.8743\n",
      "Epoch 16 - loss: 0.013129080645740032 - acc: 0.9982666666666666 - loss_val: 0.5482358336448669 - acc_val: 0.8729\n",
      "Epoch 17 - loss: 0.011640826240181923 - acc: 0.9984 - loss_val: 0.5766198635101318 - acc_val: 0.8724\n",
      "Epoch 18 - loss: 0.010524570941925049 - acc: 0.9984666666666666 - loss_val: 0.6036290526390076 - acc_val: 0.8723\n",
      "Epoch 19 - loss: 0.009707880206406116 - acc: 0.9984666666666666 - loss_val: 0.6305214166641235 - acc_val: 0.8719\n",
      "Epoch 20 - loss: 0.009077316150069237 - acc: 0.9984666666666666 - loss_val: 0.6565642356872559 - acc_val: 0.8715\n",
      "test_acc: 0.85716\n"
     ]
    }
   ],
   "source": [
    "# IMDB - 2 layer network - train + validation + test, 20 epochs (pyTorch 1.2 version)\n",
    "# notes: \n",
    "# - uses keras to fetch the dataset for consistancy with other code samples, data operations and feeding to network done with numpy\n",
    "# - accuracy for train/validation/test is measured by passing entire set through the network (eg. in tf 2.x it's mean of batch, also possible to do in pytorch)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "#network parameters\n",
    "n_input = 10000 #input size for a single sample (10000 words)\n",
    "\n",
    "#hyperparamters\n",
    "batch_size = 512\n",
    "eta = 0.001 # keras default for rmsprop = 0.001\n",
    "max_epoch = 20\n",
    "\n",
    "\n",
    "# 1. get data (using keras to download for consistancy with other code samples)\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words= n_input)\n",
    "\n",
    "#pre-process data into tensors\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test =  vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "#validation set to use during training\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "# 2. network architecture\n",
    "class IMDBModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10000, 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        a2 = self.fc2(h1)\n",
    "        h2 = self.relu2(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "    \n",
    "network = IMDBModel()\n",
    "\n",
    "# 3. select optimizer and loss\n",
    "\n",
    "optimizer = optim.RMSprop(network.parameters(), lr= eta)\n",
    "loss_criterion = nn.BCELoss()\n",
    "\n",
    "# 4. train / run network\n",
    "\n",
    "#define training procedure\n",
    "def train_step(batch_x, batch_y):\n",
    "    #zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    #forward pass\n",
    "    outputs = network(batch_x.float())\n",
    "    loss = loss_criterion(outputs, batch_y)\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #propagate/update weights\n",
    "    optimizer.step()\n",
    "\n",
    "#define accuracy\n",
    "def accuracy(model_output, labels):\n",
    "    output_mod = model_output.round().reshape(model_output.shape[0],)\n",
    "    return (output_mod == labels).sum() / len(labels)\n",
    "    \n",
    "print(\"Training with {} epochs, {} batches each ... \".format(max_epoch, int(len(partial_x_train) / batch_size) ))\n",
    "for epoch in range(max_epoch):    \n",
    "    batch_steps = int(len(partial_x_train) / batch_size)\n",
    "    for i in range(batch_steps):\n",
    "        batch_x = torch.tensor(partial_x_train[i*batch_size:(i+1)*batch_size])\n",
    "        batch_y = torch.tensor(partial_y_train[i*batch_size:(i+1)*batch_size])\n",
    "        train_step(batch_x, batch_y)\n",
    "    #check output for full train\n",
    "    output_tr = network(torch.tensor(partial_x_train).float())\n",
    "    loss_tr = loss_criterion(output_tr, torch.tensor(partial_y_train))\n",
    "    acc_tr = accuracy(output_tr.detach().numpy(), partial_y_train)\n",
    "    #check output for validate set\n",
    "    output_val = network(torch.tensor(x_val).float())\n",
    "    loss_val = loss_criterion(output_val, torch.tensor(y_val))\n",
    "    acc_val = accuracy(output_val.detach().numpy(), y_val)\n",
    "    print(\"Epoch {} - loss: {} - acc: {} - loss_val: {} - acc_val: {}\".format(epoch+1,loss_tr, acc_tr, loss_val,acc_val))\n",
    "\n",
    "# 5. test model\n",
    "acc_tst = accuracy(network(torch.tensor(x_test).float()).detach().numpy(), y_test)\n",
    "print(\"test_acc: {}\".format(acc_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
