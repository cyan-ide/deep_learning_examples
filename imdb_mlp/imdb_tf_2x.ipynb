{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function train_step at 0x000001FDD2C26DC8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <function train_step at 0x000001FDD2C26DC8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Layer imdb_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From C:\\Users\\adamw\\Anaconda3\\envs\\python_3.7_tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001FDE79CED38> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001FDE79CED38> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function test_step at 0x000001FDD2C26AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <function test_step at 0x000001FDD2C26AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "Epoch 1, loss: 0.5347306132316589 - acc: 0.7758620977401733 - val_loss: 0.4032492935657501 - val_acc: 0.8707000017166138\n",
      "Epoch 2, loss: 0.324677437543869 - acc: 0.9010640978813171 - val_loss: 0.31403589248657227 - val_acc: 0.8873000144958496\n",
      "Epoch 3, loss: 0.2354554980993271 - acc: 0.9274649620056152 - val_loss: 0.28364911675453186 - val_acc: 0.8910999894142151\n",
      "Epoch 4, loss: 0.18451696634292603 - acc: 0.941069483757019 - val_loss: 0.29726359248161316 - val_acc: 0.8769000172615051\n",
      "Epoch 5, loss: 0.15204398334026337 - acc: 0.9510371685028076 - val_loss: 0.2778233289718628 - val_acc: 0.8889999985694885\n",
      "Epoch 6, loss: 0.12382445484399796 - acc: 0.963227391242981 - val_loss: 0.3039263188838959 - val_acc: 0.8795999884605408\n",
      "Epoch 7, loss: 0.10458159446716309 - acc: 0.9684805870056152 - val_loss: 0.3159080445766449 - val_acc: 0.8827000260353088\n",
      "Epoch 8, loss: 0.08627033233642578 - acc: 0.9748787879943848 - val_loss: 0.3211798071861267 - val_acc: 0.8809000253677368\n",
      "Epoch 9, loss: 0.07328779250383377 - acc: 0.9795932173728943 - val_loss: 0.3434804379940033 - val_acc: 0.8808000087738037\n",
      "Epoch 10, loss: 0.061784107238054276 - acc: 0.984038233757019 - val_loss: 0.36659446358680725 - val_acc: 0.8755999803543091\n",
      "Epoch 11, loss: 0.05058284476399422 - acc: 0.9883486032485962 - val_loss: 0.3925650119781494 - val_acc: 0.8774999976158142\n",
      "Epoch 12, loss: 0.04171789437532425 - acc: 0.9913119673728943 - val_loss: 0.45895126461982727 - val_acc: 0.8622000217437744\n",
      "Epoch 13, loss: 0.032718393951654434 - acc: 0.994140625 - val_loss: 0.44117501378059387 - val_acc: 0.8744000196456909\n",
      "Epoch 14, loss: 0.02815704233944416 - acc: 0.994746744632721 - val_loss: 0.47224336862564087 - val_acc: 0.8704000115394592\n",
      "Epoch 15, loss: 0.022935938090085983 - acc: 0.9955549836158752 - val_loss: 0.49628862738609314 - val_acc: 0.8705000281333923\n",
      "Epoch 16, loss: 0.019157811999320984 - acc: 0.996699869632721 - val_loss: 0.5176799893379211 - val_acc: 0.8701000213623047\n",
      "Epoch 17, loss: 0.015563867054879665 - acc: 0.9979121685028076 - val_loss: 0.546593964099884 - val_acc: 0.870199978351593\n",
      "Epoch 18, loss: 0.013584332540631294 - acc: 0.997440755367279 - val_loss: 0.5719497203826904 - val_acc: 0.8680999875068665\n",
      "Epoch 19, loss: 0.01023892778903246 - acc: 0.9987877011299133 - val_loss: 0.5962319374084473 - val_acc: 0.8665000200271606\n",
      "Epoch 20, loss: 0.008340631611645222 - acc: 0.9989224076271057 - val_loss: 0.6187964081764221 - val_acc: 0.8661999702453613\n",
      "WARNING:tensorflow:Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method IMDBModel.call of <__main__.IMDBModel object at 0x000001FDBEA96148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.8502828478813171\n"
     ]
    }
   ],
   "source": [
    "# IMDB - 2 layer network - train + validation + test, 20 epochs, 512 batch size (Tensorflow 2.x version, Subclassing API)\n",
    "# notes: \n",
    "# - uses keras to fetch the dataset for consistancy with other code samples, data operations and feeding to network done with numpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#network parameters\n",
    "n_input = 10000 #input size for a single sample (10000 words)\n",
    "\n",
    "#hyperparamters\n",
    "batch_size = 512\n",
    "eta = 0.001 # learning rate\n",
    "max_epoch = 20\n",
    "\n",
    "# 1. get data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "#pre-process data into tensors\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test =  vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "#validation set to use during training\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "# 2. network architecture\n",
    "class IMDBModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        #constructor = define all layers (without connecting them)\n",
    "        super(IMDBModel, self).__init__()\n",
    "        self.fc1 = Dense(16, activation='relu')\n",
    "        self.fc2 = Dense(16, activation='relu')\n",
    "        self.out = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        #connect layers / tell the model the order of execution of layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = IMDBModel()\n",
    "\n",
    "# 3. select optimizer and loss\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "#define metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "# 4. train / run network\n",
    "\n",
    "#define training procedure\n",
    "@tf.function #this makes python code compile to tensorflow C backend\n",
    "def train_step(batch_x, batch_y):\n",
    "    #run forward pass\n",
    "    with tf.GradientTape() as tape: #gradient tape is used to \"record\" forward pass operations\n",
    "        predictions = model(batch_x) #(1) execute all layers (reference the \"call\" method of the subclassed IMDBModel)\n",
    "        loss = loss_object(y_true = batch_y, y_pred = predictions) #(2) calculate loss comparing labels vs. model output\n",
    "\n",
    "    #run backpropagation, calculate and apply gradients to adjust model weights\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    #calcualte metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(batch_y, predictions)\n",
    "\n",
    "#define test procedure\n",
    "@tf.function\n",
    "def test_step(batch_x, batch_y):\n",
    "    predictions = model(batch_x)\n",
    "    t_loss = loss_object(y_true = batch_y, y_pred = predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(batch_y, predictions)\n",
    "\n",
    "#run train    \n",
    "for epoch in range(max_epoch):\n",
    "    batch_steps = int(len(partial_x_train) / batch_size)\n",
    "    for i in range(batch_steps):\n",
    "        batch_x = partial_x_train[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = partial_y_train[i*batch_size:(i+1)*batch_size]\n",
    "        train_step(batch_x, batch_y)\n",
    "\n",
    "    #check validation accuracy\n",
    "    test_step(x_val, y_val)\n",
    "\n",
    "    template = 'Epoch {}, loss: {} - acc: {} - val_loss: {} - val_acc: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result(),\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "# 5. test model\n",
    "\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "for i in range(batch_steps):\n",
    "    batch_x = x_test[i*batch_size:(i+1)*batch_size] #, tf.newaxis\n",
    "    batch_y = y_test[i*batch_size:(i+1)*batch_size] #, tf.newaxis\n",
    "    test_step(batch_x, batch_y)\n",
    "    \n",
    "print(\"test_acc: {}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
