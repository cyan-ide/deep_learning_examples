{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adamw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adamw\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 01, Loss = 0.694392, acc: 0.502800, validation_acc: 0.489500\n",
      "Epoch 02, Loss = 0.692409, acc: 0.543800, validation_acc: 0.525300\n",
      "Epoch 03, Loss = 0.679538, acc: 0.678267, validation_acc: 0.667300\n",
      "Epoch 04, Loss = 0.576197, acc: 0.846733, validation_acc: 0.828900\n",
      "Epoch 05, Loss = 0.418976, acc: 0.896467, validation_acc: 0.867300\n",
      "Epoch 06, Loss = 0.309507, acc: 0.925467, validation_acc: 0.884700\n",
      "Epoch 07, Loss = 0.236928, acc: 0.943000, validation_acc: 0.887200\n",
      "Epoch 08, Loss = 0.192487, acc: 0.949733, validation_acc: 0.880900\n",
      "Epoch 09, Loss = 0.159984, acc: 0.966333, validation_acc: 0.886900\n",
      "Epoch 10, Loss = 0.129348, acc: 0.969133, validation_acc: 0.879700\n",
      "Epoch 11, Loss = 0.111089, acc: 0.977800, validation_acc: 0.883700\n",
      "Epoch 12, Loss = 0.092453, acc: 0.979467, validation_acc: 0.875700\n",
      "Epoch 13, Loss = 0.078888, acc: 0.985400, validation_acc: 0.880300\n",
      "Epoch 14, Loss = 0.066575, acc: 0.987133, validation_acc: 0.874300\n",
      "Epoch 15, Loss = 0.055532, acc: 0.989467, validation_acc: 0.875200\n",
      "Epoch 16, Loss = 0.045631, acc: 0.989333, validation_acc: 0.866300\n",
      "Epoch 17, Loss = 0.038575, acc: 0.994667, validation_acc: 0.875400\n",
      "Epoch 18, Loss = 0.032357, acc: 0.992067, validation_acc: 0.861600\n",
      "Epoch 19, Loss = 0.026497, acc: 0.997133, validation_acc: 0.871400\n",
      "Epoch 20, Loss = 0.020534, acc: 0.997667, validation_acc: 0.870600\n",
      "test_acc: 0.8572\n"
     ]
    }
   ],
   "source": [
    "# IMDB - 2 layer network - train + validation + test, 20 epochs, 512 batch size (Tensorflow 1.x version)\n",
    "# notes: \n",
    "# - uses keras to fetch the dataset for consistancy with other code samples, data operations and feeding to network done with numpy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#network parameters\n",
    "n_input = 10000 #input size for a single sample (10000 words)\n",
    "\n",
    "#hyperparamters\n",
    "batch_size = 512\n",
    "eta = 0.001 # learning rate\n",
    "max_epoch = 20\n",
    "\n",
    "# 1. get data (using same dataset as keras example)\n",
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=n_input)\n",
    "\n",
    "#pre-process data into tensors\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test =  vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "y_train = y_train.reshape(len(y_train),1) #reshape for format taken by tf\n",
    "y_test = y_test.reshape(len(y_test),1) #reshape for format taken by tf\n",
    "\n",
    "#validation set to use during training\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "# 2. network architecture\n",
    "def multilayer_perceptron(x):\n",
    "    #tf.reset_default_graph()\n",
    "    fc1 = layers.fully_connected(x, 16, activation_fn = tf.nn.relu, scope = 'fc1')\n",
    "    fc2 = layers.fully_connected(fc1, 16, activation_fn = tf.nn.relu, scope = 'fc2')\n",
    "    out = layers.fully_connected(fc2, 1, activation_fn = None, scope = 'out') #no tf.Sigmoid activation function as tf.loss implementation expect raw output\n",
    "    return out\n",
    "\n",
    "# 3. select optimizer and loss\n",
    "\n",
    "#input data placeholders\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name='placeholder_x')\n",
    "y = tf.placeholder(tf.float32, name='placeholder_y') \n",
    "\n",
    "#network model\n",
    "y_hat = multilayer_perceptron(x)\n",
    "#loss\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=y))\n",
    "#optimizer\n",
    "train = tf.train.RMSPropOptimizer(learning_rate= eta).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4. train / run network\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #define accuracy\n",
    "    prediction = tf.nn.sigmoid(y_hat)\n",
    "    correct_prediction = tf.equal(tf.round(prediction), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        batch_steps = int(len(partial_x_train) / batch_size)\n",
    "        for i in range(batch_steps):\n",
    "            batch_x = partial_x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = partial_y_train[i*batch_size:(i+1)*batch_size]\n",
    "            _, c = sess.run([train, loss], feed_dict = {x: batch_x, y: batch_y })\n",
    "            epoch_loss += c / batch_steps\n",
    "        validation_accuracy = accuracy.eval( {x: x_val, y: y_val}  )\n",
    "        train_accuracy = accuracy.eval( {x: partial_x_train, y: partial_y_train}  )\n",
    "        print('Epoch %02d, Loss = %.6f, acc: %.6f, validation_acc: %.6f' % (epoch+1, epoch_loss, train_accuracy, validation_accuracy))\n",
    "        \n",
    "    # 5. test model\n",
    "    print(\"test_acc:\", accuracy.eval( {x: x_test, y: y_test}  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
